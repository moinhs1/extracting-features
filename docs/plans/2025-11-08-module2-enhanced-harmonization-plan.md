# Enhanced Lab Harmonization Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Enhance Module 2 with LOINC database integration, hierarchical clustering, automated unit conversion, and interactive visualizations to achieve 90-95% lab test harmonization coverage.

**Architecture:** Three-tier cascading harmonization (LOINC exact â†’ LOINC family â†’ hierarchical clustering) with automated unit conversion and interactive plotly visualizations for review. Maintains backward compatibility with existing pipeline.

**Tech Stack:** Python 3.8+, pandas, numpy, scipy (clustering), plotly (visualizations), pint (unit validation), pickle (caching)

**Design Reference:** `docs/plans/2025-11-08-module2-enhanced-lab-harmonization-design.md`

---

## Prerequisites

Before starting:
1. LOINC database downloaded and placed at `module_2_laboratory_processing/Loinc/LoincTable/Loinc.csv`
2. Install new dependencies: `pip install scipy plotly pint`
3. Read design document: `docs/plans/2025-11-08-module2-enhanced-lab-harmonization-design.md`

---

## Task 1: Project Structure Setup

**Files:**
- Create: `module_2_laboratory_processing/loinc_matcher.py`
- Create: `module_2_laboratory_processing/cache/` (directory)
- Create: `tests/test_loinc_matcher.py`
- Modify: `module_2_laboratory_processing/requirements.txt`

**Step 1: Create project directories**

```bash
mkdir -p module_2_laboratory_processing/cache
mkdir -p tests
```

**Step 2: Update requirements.txt**

Add to `module_2_laboratory_processing/requirements.txt`:
```txt
scipy>=1.9.0
plotly>=5.14.0
pint>=0.20
kaleido>=0.2.1
```

**Step 3: Install dependencies**

```bash
cd module_2_laboratory_processing
pip install -r requirements.txt
```

Expected: All packages install successfully

**Step 4: Create empty LOINC matcher module**

Create `module_2_laboratory_processing/loinc_matcher.py`:
```python
"""
LOINC database loading and matching for lab test harmonization.
"""

import pandas as pd
import pickle
import os
from pathlib import Path
from typing import Dict, Optional


class LoincMatcher:
    """Handles LOINC database loading and matching."""

    def __init__(self, loinc_csv_path: str, cache_dir: str = 'cache'):
        self.loinc_csv_path = Path(loinc_csv_path)
        self.cache_dir = Path(cache_dir)
        self.cache_path = self.cache_dir / 'loinc_database.pkl'
        self.loinc_dict = None

    def load(self) -> Dict:
        """Load LOINC database with caching."""
        raise NotImplementedError("To be implemented")

    def match(self, loinc_code: str) -> Optional[Dict]:
        """Look up LOINC code."""
        raise NotImplementedError("To be implemented")
```

**Step 5: Create test file skeleton**

Create `tests/test_loinc_matcher.py`:
```python
"""
Tests for LOINC database loading and matching.
"""

import pytest
from pathlib import Path
import sys

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'module_2_laboratory_processing'))

from loinc_matcher import LoincMatcher


class TestLoincMatcher:
    """Test suite for LoincMatcher."""

    def test_initialization(self):
        """Test LoincMatcher initialization."""
        matcher = LoincMatcher('Loinc/LoincTable/Loinc.csv')
        assert matcher.loinc_csv_path == Path('Loinc/LoincTable/Loinc.csv')
        assert matcher.cache_dir == Path('cache')
```

**Step 6: Run test to verify setup**

```bash
cd /home/moin/TDA_11_1
pytest tests/test_loinc_matcher.py::TestLoincMatcher::test_initialization -v
```

Expected: PASS

**Step 7: Commit**

```bash
git add module_2_laboratory_processing/loinc_matcher.py
git add module_2_laboratory_processing/cache/.gitkeep
git add tests/test_loinc_matcher.py
git add module_2_laboratory_processing/requirements.txt
git commit -m "feat(module2): add LOINC matcher skeleton and project structure

- Create LoincMatcher class with load() and match() stubs
- Add cache directory for LOINC database caching
- Add new dependencies: scipy, plotly, pint
- Add initial test structure

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 2: LOINC Database Loading with Caching

**Files:**
- Modify: `module_2_laboratory_processing/loinc_matcher.py:17-34`
- Modify: `tests/test_loinc_matcher.py`

**Step 1: Write test for LOINC loading**

Add to `tests/test_loinc_matcher.py`:
```python
def test_load_from_cache(self, tmp_path):
    """Test loading LOINC from cache."""
    # Create a small mock LOINC CSV
    loinc_csv = tmp_path / "Loinc.csv"
    loinc_csv.write_text(
        '"LOINC_NUM","COMPONENT","PROPERTY","SYSTEM","CLASS","EXAMPLE_UNITS","LONG_COMMON_NAME","SHORTNAME"\n'
        '"2093-3","Cholesterol","MCnc","Ser/Plas","LABORATORY","mg/dL","Cholesterol [Mass/volume] in Serum or Plasma","Cholesterol SerPl-mCnc"\n'
    )

    cache_dir = tmp_path / "cache"
    cache_dir.mkdir()

    # First load - should parse CSV
    matcher = LoincMatcher(str(loinc_csv), cache_dir=str(cache_dir))
    result = matcher.load()

    assert result is not None
    assert '2093-3' in result
    assert result['2093-3']['component'] == 'Cholesterol'

    # Cache file should exist
    assert (cache_dir / 'loinc_database.pkl').exists()

    # Second load - should use cache
    matcher2 = LoincMatcher(str(loinc_csv), cache_dir=str(cache_dir))
    result2 = matcher2.load()

    assert result2 == result
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_loinc_matcher.py::TestLoincMatcher::test_load_from_cache -v
```

Expected: FAIL with "NotImplementedError"

**Step 3: Implement LOINC loading**

Modify `module_2_laboratory_processing/loinc_matcher.py`, replace `load()` method:
```python
def load(self) -> Dict:
    """
    Load and parse LOINC database with caching.

    Returns:
        dict: {loinc_code: {component, system, units, name, ...}}
    """
    # Check cache first
    if self.cache_path.exists():
        print(f"Loading LOINC database from cache...")
        with open(self.cache_path, 'rb') as f:
            self.loinc_dict = pickle.load(f)
        print(f"  Loaded {len(self.loinc_dict)} LOINC codes from cache")
        return self.loinc_dict

    # Parse LOINC CSV
    print(f"Parsing LOINC database from {self.loinc_csv_path}...")

    if not self.loinc_csv_path.exists():
        raise FileNotFoundError(
            f"LOINC database not found at {self.loinc_csv_path}. "
            f"Please download from https://loinc.org and place at this path."
        )

    loinc_df = pd.read_csv(self.loinc_csv_path, dtype=str, low_memory=False)

    # Filter to laboratory tests only
    loinc_df = loinc_df[loinc_df['CLASS'] == 'LABORATORY'].copy()
    print(f"  Found {len(loinc_df)} laboratory tests")

    # Build lookup dictionary
    self.loinc_dict = {}
    for _, row in loinc_df.iterrows():
        loinc_code = row['LOINC_NUM']
        self.loinc_dict[loinc_code] = {
            'component': row.get('COMPONENT', ''),
            'property': row.get('PROPERTY', ''),
            'system': row.get('SYSTEM', ''),
            'scale': row.get('SCALE_TYP', ''),
            'method': row.get('METHOD_TYP', ''),
            'units': row.get('EXAMPLE_UNITS', ''),
            'ucum_units': row.get('EXAMPLE_UCUM_UNITS', ''),
            'name': row.get('LONG_COMMON_NAME', ''),
            'short_name': row.get('SHORTNAME', ''),
            'class': row.get('CLASS', '')
        }

    # Cache for future use
    self.cache_dir.mkdir(exist_ok=True, parents=True)
    with open(self.cache_path, 'wb') as f:
        pickle.dump(self.loinc_dict, f)
    print(f"  Cached LOINC database to {self.cache_path}")

    return self.loinc_dict
```

**Step 4: Run test to verify it passes**

```bash
pytest tests/test_loinc_matcher.py::TestLoincMatcher::test_load_from_cache -v
```

Expected: PASS

**Step 5: Test with actual LOINC database (if available)**

```bash
cd module_2_laboratory_processing
python -c "
from loinc_matcher import LoincMatcher
matcher = LoincMatcher('Loinc/LoincTable/Loinc.csv')
result = matcher.load()
print(f'Loaded {len(result)} LOINC codes')
print(f'Sample: {list(result.keys())[:5]}')
"
```

Expected: Loads ~100K codes, or FileNotFoundError if LOINC not downloaded

**Step 6: Commit**

```bash
git add module_2_laboratory_processing/loinc_matcher.py
git add tests/test_loinc_matcher.py
git commit -m "feat(module2): implement LOINC database loading with caching

- Parse LOINC CSV and extract laboratory tests only
- Build lookup dictionary with component, system, units metadata
- Cache parsed data as pickle for fast subsequent loads
- Add comprehensive test with mock LOINC data

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 3: LOINC Code Matching

**Files:**
- Modify: `module_2_laboratory_processing/loinc_matcher.py:36-46`
- Modify: `tests/test_loinc_matcher.py`

**Step 1: Write test for LOINC matching**

Add to `tests/test_loinc_matcher.py`:
```python
def test_match_loinc_code(self, tmp_path):
    """Test matching LOINC code."""
    # Create mock LOINC CSV with multiple codes
    loinc_csv = tmp_path / "Loinc.csv"
    loinc_csv.write_text(
        '"LOINC_NUM","COMPONENT","PROPERTY","SYSTEM","CLASS","EXAMPLE_UNITS","LONG_COMMON_NAME","SHORTNAME"\n'
        '"2093-3","Cholesterol","MCnc","Ser/Plas","LABORATORY","mg/dL","Cholesterol [Mass/volume] in Serum or Plasma","Cholesterol SerPl-mCnc"\n'
        '"2085-9","Cholesterol.in HDL","MCnc","Ser/Plas","LABORATORY","mg/dL","Cholesterol in HDL [Mass/volume] in Serum or Plasma","HDL SerPl-mCnc"\n'
    )

    cache_dir = tmp_path / "cache"
    cache_dir.mkdir()

    matcher = LoincMatcher(str(loinc_csv), cache_dir=str(cache_dir))
    matcher.load()

    # Match existing code
    result = matcher.match('2093-3')
    assert result is not None
    assert result['component'] == 'Cholesterol'
    assert result['units'] == 'mg/dL'

    # Match different code
    result2 = matcher.match('2085-9')
    assert result2 is not None
    assert result2['component'] == 'Cholesterol.in HDL'

    # Non-existent code
    result3 = matcher.match('99999-9')
    assert result3 is None
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_loinc_matcher.py::TestLoincMatcher::test_match_loinc_code -v
```

Expected: FAIL with "NotImplementedError"

**Step 3: Implement LOINC matching**

Modify `module_2_laboratory_processing/loinc_matcher.py`, replace `match()` method:
```python
def match(self, loinc_code: str) -> Optional[Dict]:
    """
    Look up LOINC code.

    Args:
        loinc_code: LOINC code to look up (e.g., "2093-3")

    Returns:
        dict or None: LOINC metadata if found, None otherwise
    """
    if self.loinc_dict is None:
        self.load()

    return self.loinc_dict.get(loinc_code)
```

**Step 4: Run test to verify it passes**

```bash
pytest tests/test_loinc_matcher.py::TestLoincMatcher::test_match_loinc_code -v
```

Expected: PASS

**Step 5: Run all LOINC tests**

```bash
pytest tests/test_loinc_matcher.py -v
```

Expected: All tests PASS

**Step 6: Commit**

```bash
git add module_2_laboratory_processing/loinc_matcher.py
git add tests/test_loinc_matcher.py
git commit -m "feat(module2): implement LOINC code matching

- Add match() method to look up LOINC codes
- Auto-load database if not already loaded
- Return None for non-existent codes
- Add comprehensive matching tests

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 4: Unit Converter Module

**Files:**
- Create: `module_2_laboratory_processing/unit_converter.py`
- Create: `tests/test_unit_converter.py`

**Step 1: Write test for unit conversion**

Create `tests/test_unit_converter.py`:
```python
"""
Tests for unit conversion system.
"""

import pytest
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent / 'module_2_laboratory_processing'))

from unit_converter import UnitConverter


class TestUnitConverter:
    """Test suite for UnitConverter."""

    def test_glucose_conversion_mmol_to_mgdl(self):
        """Test glucose conversion from mmol/L to mg/dL."""
        converter = UnitConverter()

        # 5.5 mmol/L = 99.099 mg/dL
        converted, target_unit, success = converter.convert_value(
            value=5.5,
            test_component='glucose',
            source_unit='mmol/L'
        )

        assert success is True
        assert target_unit == 'mg/dL'
        assert abs(converted - 99.099) < 0.01

    def test_glucose_no_conversion_needed(self):
        """Test glucose when already in mg/dL."""
        converter = UnitConverter()

        converted, target_unit, success = converter.convert_value(
            value=100.0,
            test_component='glucose',
            source_unit='mg/dL'
        )

        assert success is True
        assert target_unit == 'mg/dL'
        assert converted == 100.0

    def test_unknown_component(self):
        """Test conversion for unknown test component."""
        converter = UnitConverter()

        converted, target_unit, success = converter.convert_value(
            value=5.0,
            test_component='unknown_test',
            source_unit='unknown_unit'
        )

        assert success is False
        assert converted == 5.0
        assert target_unit == 'unknown_unit'
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_unit_converter.py -v
```

Expected: FAIL with "ModuleNotFoundError: No module named 'unit_converter'"

**Step 3: Create unit converter module**

Create `module_2_laboratory_processing/unit_converter.py`:
```python
"""
Unit conversion system for lab tests.
"""

from typing import Dict, Optional, Tuple


# Common lab test unit conversions
# Format: {test_component: {'target': unit, 'factors': {source_unit: factor}}}
DEFAULT_CONVERSIONS = {
    'glucose': {
        'target': 'mg/dL',
        'factors': {
            'mmol/L': 18.018,
            'mg/dL': 1.0
        }
    },
    'creatinine': {
        'target': 'mg/dL',
        'factors': {
            'Âµmol/L': 0.0113,
            'umol/L': 0.0113,
            'mg/dL': 1.0
        }
    },
    'cholesterol': {
        'target': 'mg/dL',
        'factors': {
            'mmol/L': 38.67,
            'mg/dL': 1.0
        }
    },
    'triglycerides': {
        'target': 'mg/dL',
        'factors': {
            'mmol/L': 88.57,
            'mg/dL': 1.0
        }
    },
    'bilirubin': {
        'target': 'mg/dL',
        'factors': {
            'Âµmol/L': 0.0585,
            'umol/L': 0.0585,
            'mg/dL': 1.0
        }
    },
    'calcium': {
        'target': 'mg/dL',
        'factors': {
            'mmol/L': 4.008,
            'mg/dL': 1.0
        }
    },
}


class UnitConverter:
    """Handles unit conversion for lab tests."""

    def __init__(self, custom_conversions: Optional[Dict] = None):
        """
        Initialize unit converter.

        Args:
            custom_conversions: Optional dict of custom conversions to add
        """
        self.conversions = DEFAULT_CONVERSIONS.copy()
        if custom_conversions:
            self.conversions.update(custom_conversions)

    def normalize_unit(self, unit: str) -> str:
        """
        Normalize unit string for matching.

        Args:
            unit: Unit string to normalize

        Returns:
            str: Normalized unit
        """
        # Convert to lowercase and strip
        unit = unit.lower().strip()

        # Remove spaces
        unit = unit.replace(' ', '')

        # Common mappings
        normalize_map = {
            'mgdl': 'mg/dL',
            'mg/dl': 'mg/dL',
            'mmol': 'mmol/L',
            'umol/l': 'Âµmol/L',
            'umoll': 'Âµmol/L',
            'g/dl': 'g/dL',
            'gdl': 'g/dL',
            'u/l': 'U/L',
            'ul': 'U/L'
        }

        return normalize_map.get(unit, unit)

    def get_conversion_factor(
        self,
        test_component: str,
        source_unit: str
    ) -> Optional[float]:
        """
        Get conversion factor from source unit to target unit.

        Args:
            test_component: Test component name (e.g., 'glucose')
            source_unit: Source unit (e.g., 'mmol/L')

        Returns:
            float or None: Conversion factor, or None if not found
        """
        # Normalize inputs
        component = test_component.lower().strip()
        unit = self.normalize_unit(source_unit)

        # Look up conversion
        if component in self.conversions:
            factors = self.conversions[component]['factors']
            # Try normalized unit, then original
            return factors.get(unit) or factors.get(source_unit)

        return None

    def convert_value(
        self,
        value: float,
        test_component: str,
        source_unit: str
    ) -> Tuple[float, str, bool]:
        """
        Convert a value to standard units.

        Args:
            value: Original value
            test_component: Test component name
            source_unit: Source unit

        Returns:
            (converted_value, target_unit, success):
                - converted_value: Converted value, or original if no conversion
                - target_unit: Target unit, or source if no conversion
                - success: True if conversion applied, False if not found
        """
        factor = self.get_conversion_factor(test_component, source_unit)

        if factor is None:
            # No conversion found, return original
            return value, source_unit, False

        # Apply conversion
        converted = value * factor

        # Get target unit
        component = test_component.lower().strip()
        target_unit = self.conversions[component]['target']

        return converted, target_unit, True
```

**Step 4: Run test to verify it passes**

```bash
pytest tests/test_unit_converter.py -v
```

Expected: All tests PASS

**Step 5: Test additional conversions**

Add to `tests/test_unit_converter.py`:
```python
def test_creatinine_conversion(self):
    """Test creatinine conversion from Âµmol/L to mg/dL."""
    converter = UnitConverter()

    # 88.4 Âµmol/L = 1.0 mg/dL
    converted, target_unit, success = converter.convert_value(
        value=88.4,
        test_component='creatinine',
        source_unit='Âµmol/L'
    )

    assert success is True
    assert target_unit == 'mg/dL'
    assert abs(converted - 0.999) < 0.01

def test_cholesterol_conversion(self):
    """Test cholesterol conversion from mmol/L to mg/dL."""
    converter = UnitConverter()

    # 5.0 mmol/L = 193.35 mg/dL
    converted, target_unit, success = converter.convert_value(
        value=5.0,
        test_component='cholesterol',
        source_unit='mmol/L'
    )

    assert success is True
    assert target_unit == 'mg/dL'
    assert abs(converted - 193.35) < 0.01
```

**Step 6: Run all tests**

```bash
pytest tests/test_unit_converter.py -v
```

Expected: All tests PASS

**Step 7: Commit**

```bash
git add module_2_laboratory_processing/unit_converter.py
git add tests/test_unit_converter.py
git commit -m "feat(module2): add unit conversion system

- Implement UnitConverter with default conversions for common tests
- Support glucose, creatinine, cholesterol, triglycerides, bilirubin, calcium
- Normalize unit strings (mg/dl â†’ mg/dL, etc.)
- Return success flag for conversion attempts
- Add comprehensive test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 5: Hierarchical Clustering - Distance Metrics

**Files:**
- Create: `module_2_laboratory_processing/hierarchical_clustering.py`
- Create: `tests/test_hierarchical_clustering.py`

**Step 1: Write test for token similarity**

Create `tests/test_hierarchical_clustering.py`:
```python
"""
Tests for hierarchical clustering.
"""

import pytest
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent / 'module_2_laboratory_processing'))

from hierarchical_clustering import (
    calculate_token_similarity,
    calculate_unit_incompatibility,
    calculate_combined_distance
)


class TestDistanceMetrics:
    """Test distance metrics for clustering."""

    def test_token_similarity_identical(self):
        """Test token similarity for identical names."""
        sim = calculate_token_similarity(
            "LOW DENSITY LIPOPROTEIN",
            "LOW DENSITY LIPOPROTEIN"
        )
        assert sim == 1.0

    def test_token_similarity_different_modifiers(self):
        """Test LDL vs HDL (share LIPOPROTEIN but different modifiers)."""
        sim = calculate_token_similarity(
            "LOW DENSITY LIPOPROTEIN",
            "HIGH DENSITY LIPOPROTEIN"
        )
        # Intersection: {DENSITY, LIPOPROTEIN} = 2
        # Union: {LOW, HIGH, DENSITY, LIPOPROTEIN} = 4
        # Similarity: 2/4 = 0.5
        assert sim == 0.5

    def test_token_similarity_completely_different(self):
        """Test completely different test names."""
        sim = calculate_token_similarity(
            "GLUCOSE",
            "HEMOGLOBIN"
        )
        assert sim == 0.0

    def test_unit_incompatibility_same_unit(self):
        """Test unit incompatibility for same unit."""
        incomp = calculate_unit_incompatibility("mg/dL", "mg/dL")
        assert incomp == 0.0

    def test_unit_incompatibility_convertible(self):
        """Test unit incompatibility for convertible units."""
        # Will use fallback since pint may not be installed
        incomp = calculate_unit_incompatibility("mg/dL", "mmol/L")
        # Both are concentration units, should be 0.5 in fallback
        assert 0.0 <= incomp <= 1.0
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_hierarchical_clustering.py::TestDistanceMetrics::test_token_similarity_identical -v
```

Expected: FAIL with "ModuleNotFoundError"

**Step 3: Create hierarchical clustering module with distance metrics**

Create `module_2_laboratory_processing/hierarchical_clustering.py`:
```python
"""
Hierarchical clustering for unmapped lab tests.
"""

import numpy as np
import re
from typing import Dict, List, Tuple, Set


def calculate_token_similarity(name1: str, name2: str) -> float:
    """
    Calculate Jaccard similarity between tokenized test names.

    Args:
        name1: First test name (e.g., "LOW DENSITY LIPOPROTEIN")
        name2: Second test name (e.g., "HIGH DENSITY LIPOPROTEIN")

    Returns:
        float: Similarity in [0, 1], where 1=identical tokens
    """
    # Tokenize: split on whitespace, convert to uppercase, remove empty
    tokens1 = set(name1.upper().split())
    tokens2 = set(name2.upper().split())

    # Remove common stop words that don't add meaning
    stop_words = {'TEST', 'BLOOD', 'SERUM', 'PLASMA', 'LEVEL', 'TOTAL'}
    tokens1 = tokens1 - stop_words
    tokens2 = tokens2 - stop_words

    # Jaccard similarity
    intersection = tokens1 & tokens2
    union = tokens1 | tokens2

    if len(union) == 0:
        return 0.0

    return len(intersection) / len(union)


def calculate_unit_incompatibility(unit1: str, unit2: str) -> float:
    """
    Calculate unit incompatibility score.

    Args:
        unit1: First unit (e.g., "mg/dL")
        unit2: Second unit (e.g., "mmol/L")

    Returns:
        float: Incompatibility in [0, 1], where 0=same unit, 1=incompatible
    """
    # Normalize units (lowercase, strip whitespace)
    u1 = unit1.lower().strip() if unit1 else ""
    u2 = unit2.lower().strip() if unit2 else ""

    # Exact match
    if u1 == u2:
        return 0.0

    # Try pint for dimension analysis
    try:
        import pint
        ureg = pint.UnitRegistry()

        # Parse units
        unit_obj1 = ureg(u1)
        unit_obj2 = ureg(u2)

        # Same dimensionality = convertible
        if unit_obj1.dimensionality == unit_obj2.dimensionality:
            return 0.3  # Compatible but need conversion
        else:
            return 1.0  # Incompatible dimensions

    except:
        # Fallback: simple string matching for common patterns
        normalize_map = {
            'mgdl': 'mg/dL',
            'mg/dl': 'mg/dL',
            'mmol': 'mmol/L',
            'umol/l': 'Âµmol/L',
            'g/dl': 'g/dL',
            'u/l': 'U/L'
        }

        u1_norm = normalize_map.get(u1.replace(' ', ''), u1)
        u2_norm = normalize_map.get(u2.replace(' ', ''), u2)

        if u1_norm == u2_norm:
            return 0.0

        # Check if both are concentration units (likely convertible)
        conc_units = {'mg/dL', 'mmol/L', 'Âµmol/L', 'g/dL', 'ng/mL', 'pg/mL'}
        if u1_norm in conc_units and u2_norm in conc_units:
            return 0.5  # Possibly convertible, needs review

        # Otherwise, incompatible
        return 1.0


def calculate_combined_distance(
    test1: Dict,
    test2: Dict,
    unit_weight: float = 0.4
) -> float:
    """
    Combined distance metric: 60% name similarity + 40% unit compatibility.

    Args:
        test1: Test dictionary with 'name' and 'unit' keys
        test2: Test dictionary with 'name' and 'unit' keys
        unit_weight: Weight for unit incompatibility (default 0.4)

    Returns:
        float: Distance in [0, 1], where 0=identical, 1=completely different
    """
    # Calculate name similarity and convert to distance
    name_similarity = calculate_token_similarity(test1['name'], test2['name'])
    name_distance = 1 - name_similarity

    # Calculate unit incompatibility (already a distance)
    unit_distance = calculate_unit_incompatibility(test1['unit'], test2['unit'])

    # Weighted combination
    combined = (1 - unit_weight) * name_distance + unit_weight * unit_distance

    return combined
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_hierarchical_clustering.py::TestDistanceMetrics -v
```

Expected: All tests PASS

**Step 5: Add test for combined distance**

Add to `tests/test_hierarchical_clustering.py`:
```python
def test_combined_distance_identical(self):
    """Test combined distance for identical tests."""
    test1 = {'name': 'GLUCOSE', 'unit': 'mg/dL'}
    test2 = {'name': 'GLUCOSE', 'unit': 'mg/dL'}

    dist = calculate_combined_distance(test1, test2)
    assert dist == 0.0

def test_combined_distance_ldl_vs_hdl(self):
    """Test LDL vs HDL (similar names but should not group)."""
    test1 = {'name': 'LOW DENSITY LIPOPROTEIN', 'unit': 'mg/dL'}
    test2 = {'name': 'HIGH DENSITY LIPOPROTEIN', 'unit': 'mg/dL'}

    dist = calculate_combined_distance(test1, test2)

    # Name similarity: 0.5 (share 2 of 4 tokens)
    # Name distance: 0.5
    # Unit incompatibility: 0.0 (same unit)
    # Combined: 0.6 * 0.5 + 0.4 * 0.0 = 0.3
    assert abs(dist - 0.3) < 0.01
```

**Step 6: Run all clustering tests**

```bash
pytest tests/test_hierarchical_clustering.py -v
```

Expected: All tests PASS

**Step 7: Commit**

```bash
git add module_2_laboratory_processing/hierarchical_clustering.py
git add tests/test_hierarchical_clustering.py
git commit -m "feat(module2): add distance metrics for hierarchical clustering

- Implement token-based name similarity (Jaccard index)
- Implement unit incompatibility with pint fallback
- Implement combined distance metric (60% name, 40% unit)
- Add comprehensive test coverage for distance calculations

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 6: Hierarchical Clustering - Clustering Algorithm

**Files:**
- Modify: `module_2_laboratory_processing/hierarchical_clustering.py`
- Modify: `tests/test_hierarchical_clustering.py`

**Step 1: Write test for clustering**

Add to `tests/test_hierarchical_clustering.py`:
```python
class TestHierarchicalClustering:
    """Test hierarchical clustering algorithm."""

    def test_cluster_similar_tests(self):
        """Test clustering groups similar tests together."""
        from hierarchical_clustering import perform_hierarchical_clustering

        # Sample tests: glucose variants and cholesterol variants
        tests = [
            {'name': 'GLUCOSE', 'unit': 'mg/dL'},
            {'name': 'GLUCOSE BLOOD', 'unit': 'mg/dL'},
            {'name': 'GLUCOSE POC', 'unit': 'mg/dL'},
            {'name': 'CHOLESTEROL', 'unit': 'mg/dL'},
            {'name': 'CHOLESTEROL TOTAL', 'unit': 'mg/dL'},
        ]

        clusters, linkage_matrix, distances = perform_hierarchical_clustering(
            tests,
            threshold=0.9
        )

        # Should have 2 clusters: glucose group and cholesterol group
        assert len(clusters) >= 2

        # Glucose tests should be in same cluster
        glucose_indices = {0, 1, 2}  # First 3 tests
        cholesterol_indices = {3, 4}  # Last 2 tests

        # Find which cluster contains glucose tests
        glucose_cluster_id = None
        for cluster_id, test_indices in clusters.items():
            if 0 in test_indices:
                glucose_cluster_id = cluster_id
                # Should contain all glucose tests
                assert set(test_indices) & glucose_indices == glucose_indices
                break

        assert glucose_cluster_id is not None
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_hierarchical_clustering.py::TestHierarchicalClustering::test_cluster_similar_tests -v
```

Expected: FAIL with "ImportError: cannot import name 'perform_hierarchical_clustering'"

**Step 3: Implement hierarchical clustering**

Add to `module_2_laboratory_processing/hierarchical_clustering.py`:
```python
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform


def perform_hierarchical_clustering(
    unmapped_tests: List[Dict],
    threshold: float = 0.9,
    unit_weight: float = 0.4
) -> Tuple[Dict, np.ndarray, np.ndarray]:
    """
    Cluster unmapped tests using Ward's method with combined distance metric.

    Args:
        unmapped_tests: List of test dictionaries with 'name', 'unit', etc.
        threshold: Similarity threshold for cutting dendrogram (0-1)
        unit_weight: Weight for unit compatibility in distance (default 0.4)

    Returns:
        clusters: Dict mapping cluster_id to list of test indices
        linkage_matrix: Linkage matrix for dendrogram plotting
        distances: Full distance matrix for heatmap
    """
    n = len(unmapped_tests)

    if n == 0:
        return {}, None, None

    if n == 1:
        return {0: [0]}, None, None

    # Calculate pairwise distance matrix
    distances = np.zeros((n, n))
    for i in range(n):
        for j in range(i+1, n):
            dist = calculate_combined_distance(
                unmapped_tests[i],
                unmapped_tests[j],
                unit_weight=unit_weight
            )
            distances[i, j] = dist
            distances[j, i] = dist

    # Convert to condensed distance matrix for scipy
    condensed_dist = squareform(distances)

    # Perform hierarchical clustering (Ward's method)
    linkage_matrix = linkage(condensed_dist, method='ward')

    # Cut dendrogram at threshold
    # Ward uses squared Euclidean distance, need to adjust threshold
    distance_threshold = (1 - threshold) * linkage_matrix[-1, 2]

    # Get cluster labels
    cluster_labels = fcluster(linkage_matrix, distance_threshold, criterion='distance')

    # Group tests by cluster
    clusters = {}
    for idx, label in enumerate(cluster_labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(idx)

    return clusters, linkage_matrix, distances
```

**Step 4: Run test to verify it passes**

```bash
pytest tests/test_hierarchical_clustering.py::TestHierarchicalClustering::test_cluster_similar_tests -v
```

Expected: PASS

**Step 5: Add test for singleton clusters**

Add to `tests/test_hierarchical_clustering.py`:
```python
def test_singleton_cluster(self):
    """Test that dissimilar test creates singleton cluster."""
    from hierarchical_clustering import perform_hierarchical_clustering

    tests = [
        {'name': 'GLUCOSE', 'unit': 'mg/dL'},
        {'name': 'GLUCOSE BLOOD', 'unit': 'mg/dL'},
        {'name': 'COMPLETELY DIFFERENT TEST', 'unit': '%'},  # Very different
    ]

    clusters, _, _ = perform_hierarchical_clustering(tests, threshold=0.9)

    # Third test should be in its own cluster
    singleton_found = False
    for cluster_id, test_indices in clusters.items():
        if len(test_indices) == 1 and 2 in test_indices:
            singleton_found = True
            break

    assert singleton_found
```

**Step 6: Run all clustering tests**

```bash
pytest tests/test_hierarchical_clustering.py -v
```

Expected: All tests PASS

**Step 7: Commit**

```bash
git add module_2_laboratory_processing/hierarchical_clustering.py
git add tests/test_hierarchical_clustering.py
git commit -m "feat(module2): implement hierarchical clustering algorithm

- Use scipy Ward's method for clustering
- Calculate pairwise distance matrix with combined metric
- Cut dendrogram at similarity threshold
- Return clusters, linkage matrix, and distance matrix
- Handle edge cases (empty, singleton)
- Add comprehensive test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 7: Cluster Quality - Isoenzyme and Suspicious Pattern Detection

**Files:**
- Modify: `module_2_laboratory_processing/hierarchical_clustering.py`
- Modify: `tests/test_hierarchical_clustering.py`

**Step 1: Write test for isoenzyme detection**

Add to `tests/test_hierarchical_clustering.py`:
```python
class TestClusterQuality:
    """Test cluster quality checks."""

    def test_detect_isoenzyme_pattern_ldh(self):
        """Test detection of LDH isoenzymes."""
        from hierarchical_clustering import detect_isoenzyme_pattern

        # LDH isoenzymes
        ldh_tests = ['LDH1', 'LDH2', 'LDH3']
        assert detect_isoenzyme_pattern(ldh_tests) is True

        # Single LDH test - not a pattern
        single_ldh = ['LDH1']
        assert detect_isoenzyme_pattern(single_ldh) is False

        # Non-isoenzyme tests
        other_tests = ['GLUCOSE', 'CHOLESTEROL']
        assert detect_isoenzyme_pattern(other_tests) is False

    def test_detect_isoenzyme_pattern_ck(self):
        """Test detection of CK isoenzymes."""
        from hierarchical_clustering import detect_isoenzyme_pattern

        ck_tests = ['CK-MB', 'CK-MM']
        assert detect_isoenzyme_pattern(ck_tests) is True

    def test_detect_isoenzyme_pattern_troponin(self):
        """Test detection of troponin I vs T."""
        from hierarchical_clustering import detect_isoenzyme_pattern

        trop_tests = ['TROPONIN I', 'TROPONIN T']
        assert detect_isoenzyme_pattern(trop_tests) is True
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_hierarchical_clustering.py::TestClusterQuality::test_detect_isoenzyme_pattern_ldh -v
```

Expected: FAIL with "ImportError: cannot import name 'detect_isoenzyme_pattern'"

**Step 3: Implement isoenzyme detection**

Add to `module_2_laboratory_processing/hierarchical_clustering.py`:
```python
def detect_isoenzyme_pattern(test_names: List[str]) -> bool:
    """
    Detect if cluster contains isoenzymes (should be separated).

    Args:
        test_names: List of test names in cluster

    Returns:
        bool: True if isoenzyme pattern detected
    """
    # Patterns for isoenzymes
    patterns = [
        r'LDH\s*[1-5]',           # LDH1, LDH2, ..., LDH5
        r'CK[-\s]?(MB|MM|BB)',    # CK-MB, CK-MM, CK-BB
        r'TROPONIN\s*[IT]',       # Troponin I, Troponin T (different biomarkers)
    ]

    for pattern in patterns:
        matches = [re.search(pattern, name, re.IGNORECASE) for name in test_names]
        if sum(1 for m in matches if m) >= 2:
            return True  # At least 2 isoenzymes in cluster

    return False
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_hierarchical_clustering.py::TestClusterQuality -v
```

Expected: All tests PASS

**Step 5: Write test for flagging suspicious clusters**

Add to `tests/test_hierarchical_clustering.py`:
```python
def test_flag_isoenzyme_cluster(self):
    """Test flagging cluster with isoenzymes."""
    from hierarchical_clustering import flag_suspicious_clusters

    clusters = {
        1: [0, 1],  # LDH1 and LDH2
        2: [2, 3],  # Glucose tests
    }

    tests = [
        {'name': 'LDH1', 'unit': 'U/L'},
        {'name': 'LDH2', 'unit': 'U/L'},
        {'name': 'GLUCOSE', 'unit': 'mg/dL'},
        {'name': 'GLUCOSE BLOOD', 'unit': 'mg/dL'},
    ]

    flags = flag_suspicious_clusters(clusters, tests)

    # Cluster 1 should be flagged for isoenzyme pattern
    assert 1 in flags
    assert 'isoenzyme_pattern' in flags[1]

    # Cluster 2 should not be flagged
    assert 2 not in flags

def test_flag_large_cluster(self):
    """Test flagging very large cluster."""
    from hierarchical_clustering import flag_suspicious_clusters

    clusters = {
        1: list(range(15)),  # 15 tests - too large
    }

    tests = [{'name': f'TEST{i}', 'unit': 'mg/dL'} for i in range(15)]

    flags = flag_suspicious_clusters(clusters, tests)

    assert 1 in flags
    assert any('large_cluster' in flag for flag in flags[1])
```

**Step 6: Run test to verify it fails**

```bash
pytest tests/test_hierarchical_clustering.py::TestClusterQuality::test_flag_isoenzyme_cluster -v
```

Expected: FAIL with "ImportError: cannot import name 'flag_suspicious_clusters'"

**Step 7: Implement cluster flagging**

Add to `module_2_laboratory_processing/hierarchical_clustering.py`:
```python
def flag_suspicious_clusters(
    clusters: Dict[int, List[int]],
    unmapped_tests: List[Dict]
) -> Dict[int, List[str]]:
    """
    Identify clusters that need manual review.

    Args:
        clusters: Dict mapping cluster_id to list of test indices
        unmapped_tests: List of test dictionaries

    Returns:
        flags: Dict mapping cluster_id to list of flag reasons
    """
    flags = {}

    for cluster_id, test_indices in clusters.items():
        cluster_flags = []

        # Get test info
        test_names = [unmapped_tests[i]['name'] for i in test_indices]
        test_units = [unmapped_tests[i]['unit'] for i in test_indices]

        # Flag 1: Very large cluster (>10 tests)
        if len(test_indices) > 10:
            cluster_flags.append(f"large_cluster ({len(test_indices)} tests)")

        # Flag 2: Isoenzyme pattern
        if detect_isoenzyme_pattern(test_names):
            cluster_flags.append("isoenzyme_pattern")

        # Flag 3: Unit mismatch (incompatible units)
        unique_units = set(test_units)
        if len(unique_units) > 1:
            # Check if all are convertible
            incompatible = False
            units_list = list(unique_units)
            for i in range(len(units_list)):
                for j in range(i+1, len(units_list)):
                    incomp = calculate_unit_incompatibility(units_list[i], units_list[j])
                    if incomp > 0.5:  # Not convertible
                        incompatible = True
                        break
                if incompatible:
                    break

            if incompatible:
                cluster_flags.append(f"unit_mismatch ({', '.join(unique_units)})")

        # Flag 4: Generic terms (likely too broad)
        generic_terms = ['PANEL', 'PROFILE', 'COMPREHENSIVE', 'BASIC', 'COMPLETE']
        if any(term in ' '.join(test_names).upper() for term in generic_terms):
            cluster_flags.append("generic_terms")

        if cluster_flags:
            flags[cluster_id] = cluster_flags

    return flags
```

**Step 8: Run all tests**

```bash
pytest tests/test_hierarchical_clustering.py -v
```

Expected: All tests PASS

**Step 9: Commit**

```bash
git add module_2_laboratory_processing/hierarchical_clustering.py
git add tests/test_hierarchical_clustering.py
git commit -m "feat(module2): add cluster quality checks

- Detect isoenzyme patterns (LDH1-5, CK-MB/MM, Troponin I/T)
- Flag large clusters (>10 tests)
- Flag unit mismatches (incompatible dimensions)
- Flag generic terms (PANEL, PROFILE, etc.)
- Add comprehensive test coverage for quality checks

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 8: Integrate LOINC Tier 1 Matching into Phase 1

**Files:**
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`
- Test: Run Phase 1 with n=10

**Step 1: Add imports and constants**

Add to top of `module_02_laboratory_processing.py` (after existing imports):
```python
from loinc_matcher import LoincMatcher
from unit_converter import UnitConverter
from hierarchical_clustering import (
    perform_hierarchical_clustering,
    flag_suspicious_clusters
)
```

Add to constants section (after existing constants):
```python
# LOINC database path
LOINC_CSV_PATH = Path(__file__).parent / 'Loinc' / 'LoincTable' / 'Loinc.csv'
LOINC_CACHE_DIR = Path(__file__).parent / 'cache'
```

**Step 2: Create function for Tier 1 LOINC matching**

Add new function after `group_by_loinc()` function:
```python
def tier1_loinc_exact_match(frequency_df, loinc_matcher, unit_converter):
    """
    Tier 1: LOINC exact matching with unit validation.

    Args:
        frequency_df: Test frequency DataFrame with loinc_code column
        loinc_matcher: LoincMatcher instance
        unit_converter: UnitConverter instance

    Returns:
        pd.DataFrame: Tier 1 matches (auto-approved)
        set: Matched test descriptions
    """
    print(f"\n{'='*80}")
    print("TIER 1: LOINC EXACT MATCHING")
    print(f"{'='*80}\n")

    tier1_matches = []
    matched_tests = set()

    # Filter to tests with LOINC codes
    has_loinc = frequency_df[frequency_df['loinc_code'].notna()].copy()
    print(f"  Tests with LOINC codes: {len(has_loinc)}")

    for _, row in has_loinc.iterrows():
        loinc_code = row['loinc_code']
        test_desc = row['test_description']

        # Look up in LOINC database
        loinc_data = loinc_matcher.match(loinc_code)

        if loinc_data is None:
            continue  # LOINC code not in database

        # Extract metadata
        component = loinc_data.get('component', '')
        system = loinc_data.get('system', '')
        loinc_units = loinc_data.get('units', '')
        loinc_name = loinc_data.get('name', '')

        # Create group name from component
        group_name = component.lower().replace('.', '_').replace(' ', '_').replace(',', '')
        if not group_name:
            group_name = test_desc.lower().replace(' ', '_')

        # Get conversion factor if needed
        test_units = row['reference_units']
        conversion_factors = {test_units: 1.0}

        if test_units != loinc_units and loinc_units:
            # Try to get conversion factor
            factor = unit_converter.get_conversion_factor(
                component.split('.')[0].lower(),  # Base component
                test_units
            )
            if factor:
                conversion_factors[test_units] = factor

        tier1_matches.append({
            'group_name': group_name,
            'loinc_code': loinc_code,
            'component': component,
            'system': system,
            'standard_unit': loinc_units or test_units,
            'source_units': test_units,
            'conversion_factors': str(conversion_factors),
            'tier': 1,
            'needs_review': False,
            'review_reason': '',
            'matched_tests': test_desc,
            'patient_count': row['patient_count'],
            'measurement_count': row['count']
        })

        matched_tests.add(test_desc)

    tier1_df = pd.DataFrame(tier1_matches)
    tier1_df = tier1_df.sort_values('patient_count', ascending=False).reset_index(drop=True)

    print(f"  Tier 1 matches: {len(tier1_df)}")
    print(f"  Coverage: {len(matched_tests)}/{len(frequency_df)} ({len(matched_tests)/len(frequency_df)*100:.1f}%)\n")

    return tier1_df, matched_tests
```

**Step 3: Test Tier 1 matching**

Run Phase 1 with test mode:
```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected output should show:
```
================================================================================
TIER 1: LOINC EXACT MATCHING
================================================================================

  Tests with LOINC codes: [number]
  Tier 1 matches: [number]
  Coverage: [number]/[total] ([percentage]%)
```

**Step 4: Verify Tier 1 output file**

Check that `outputs/discovery/test_n10_tier1_loinc_exact.csv` was created:
```bash
ls -lh outputs/discovery/test_n10_tier1_loinc_exact.csv
head -5 outputs/discovery/test_n10_tier1_loinc_exact.csv
```

**Step 5: Commit**

```bash
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): integrate Tier 1 LOINC exact matching into Phase 1

- Add tier1_loinc_exact_match() function
- Look up LOINC codes in database
- Extract component, system, units metadata
- Generate group names from LOINC component
- Check for unit conversion needs
- Save tier1_loinc_exact.csv output
- Track matched tests for Tier 2/3 filtering

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 9: Integrate LOINC Tier 2 Family Matching

**Files:**
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Create Tier 2 family matching function**

Add after `tier1_loinc_exact_match()` function:
```python
def tier2_loinc_family_match(frequency_df, tier1_matched, loinc_matcher):
    """
    Tier 2: LOINC family matching (group by component).

    Args:
        frequency_df: Test frequency DataFrame
        tier1_matched: Set of tests already matched in Tier 1
        loinc_matcher: LoincMatcher instance

    Returns:
        pd.DataFrame: Tier 2 matches (needs review)
        set: Matched test descriptions
    """
    print(f"\n{'='*80}")
    print("TIER 2: LOINC FAMILY MATCHING")
    print(f"{'='*80}\n")

    # Get tests not yet matched with LOINC codes
    has_loinc = frequency_df[
        (frequency_df['loinc_code'].notna()) &
        (~frequency_df['test_description'].isin(tier1_matched))
    ].copy()

    print(f"  Tests with LOINC codes (unmapped in Tier 1): {len(has_loinc)}")

    # Group by component
    component_groups = {}
    for _, row in has_loinc.iterrows():
        loinc_code = row['loinc_code']
        loinc_data = loinc_matcher.match(loinc_code)

        if loinc_data is None:
            continue

        component = loinc_data.get('component', '')
        if not component:
            continue

        if component not in component_groups:
            component_groups[component] = []

        component_groups[component].append({
            'test_description': row['test_description'],
            'loinc_code': loinc_code,
            'system': loinc_data.get('system', ''),
            'units': row['reference_units'],
            'patient_count': row['patient_count'],
            'count': row['count']
        })

    # Create Tier 2 matches
    tier2_matches = []
    matched_tests = set()

    for component, tests in component_groups.items():
        # Check if all tests have same system
        systems = set(t['system'] for t in tests)
        units = set(t['units'] for t in tests)

        needs_review = len(systems) > 1 or len(units) > 1
        review_reason = []
        if len(systems) > 1:
            review_reason.append(f"multiple_systems ({', '.join(systems)})")
        if len(units) > 1:
            review_reason.append(f"multiple_units ({', '.join(units)})")

        group_name = component.lower().replace('.', '_').replace(' ', '_').replace(',', '')

        tier2_matches.append({
            'group_name': group_name,
            'loinc_code': tests[0]['loinc_code'],  # Representative code
            'component': component,
            'system': '|'.join(systems),
            'standard_unit': tests[0]['units'],
            'source_units': '|'.join(units),
            'conversion_factors': '{}',
            'tier': 2,
            'needs_review': needs_review,
            'review_reason': '|'.join(review_reason) if review_reason else '',
            'matched_tests': '|'.join(t['test_description'] for t in tests),
            'patient_count': max(t['patient_count'] for t in tests),
            'measurement_count': sum(t['count'] for t in tests)
        })

        for t in tests:
            matched_tests.add(t['test_description'])

    tier2_df = pd.DataFrame(tier2_matches)
    if len(tier2_df) > 0:
        tier2_df = tier2_df.sort_values('patient_count', ascending=False).reset_index(drop=True)

    print(f"  Tier 2 family groups: {len(tier2_df)}")
    print(f"  Tests matched: {len(matched_tests)}")
    print(f"  Groups needing review: {tier2_df['needs_review'].sum() if len(tier2_df) > 0 else 0}\n")

    return tier2_df, matched_tests
```

**Step 2: Integrate Tier 2 into Phase 1 workflow**

Modify `run_phase1()` function, after calling `tier1_loinc_exact_match()`:
```python
# Tier 2: LOINC family matching
tier2_df, tier2_matched = tier2_loinc_family_match(
    frequency_df,
    tier1_matched,
    loinc_matcher
)

# Save Tier 2 output
tier2_output = output_dir / f'{prefix}_tier2_loinc_family.csv'
tier2_df.to_csv(tier2_output, index=False)
print(f"  Saved Tier 2 matches to: {tier2_output}")

# Combine matched sets
all_matched = tier1_matched | tier2_matched
```

**Step 3: Test Tier 2 matching**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: Should see Tier 2 output in console and `tier2_loinc_family.csv` file created

**Step 4: Verify output**

```bash
ls -lh outputs/discovery/test_n10_tier2_loinc_family.csv
head -5 outputs/discovery/test_n10_tier2_loinc_family.csv
```

**Step 5: Commit**

```bash
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): integrate Tier 2 LOINC family matching

- Group tests by LOINC component
- Flag groups with multiple systems or units
- Generate tier2_loinc_family.csv output
- Track matched tests for Tier 3 filtering

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 10: Integrate Tier 3 Hierarchical Clustering

**Files:**
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Create Tier 3 clustering function**

Add after `tier2_loinc_family_match()` function:
```python
def tier3_hierarchical_clustering(frequency_df, all_matched, threshold=0.9):
    """
    Tier 3: Hierarchical clustering for unmapped tests.

    Args:
        frequency_df: Test frequency DataFrame
        all_matched: Set of tests already matched in Tier 1/2
        threshold: Similarity threshold for clustering (default 0.9)

    Returns:
        pd.DataFrame: Tier 3 cluster suggestions
        dict: Clusters mapping
        np.ndarray: Linkage matrix for dendrogram
        np.ndarray: Distance matrix for heatmap
    """
    print(f"\n{'='*80}")
    print("TIER 3: HIERARCHICAL CLUSTERING")
    print(f"{'='*80}\n")

    # Get unmapped tests
    unmapped_df = frequency_df[~frequency_df['test_description'].isin(all_matched)].copy()
    print(f"  Unmapped tests: {len(unmapped_df)}")

    if len(unmapped_df) == 0:
        print("  No unmapped tests to cluster\n")
        return pd.DataFrame(), {}, None, None

    # Prepare test data for clustering
    unmapped_tests = []
    for _, row in unmapped_df.iterrows():
        unmapped_tests.append({
            'name': row['test_description'],
            'unit': row['reference_units'],
            'patient_count': row['patient_count'],
            'count': row['count']
        })

    # Perform hierarchical clustering
    clusters, linkage_matrix, distances = perform_hierarchical_clustering(
        unmapped_tests,
        threshold=threshold
    )

    print(f"  Clusters found: {len(clusters)}")

    # Flag suspicious clusters
    flags = flag_suspicious_clusters(clusters, unmapped_tests)
    print(f"  Clusters flagged for review: {len(flags)}")

    # Create Tier 3 matches
    tier3_matches = []

    for cluster_id, test_indices in clusters.items():
        # Get tests in this cluster
        cluster_tests = [unmapped_tests[i] for i in test_indices]
        test_names = [t['name'] for t in cluster_tests]
        test_units = [t['unit'] for t in cluster_tests]

        # Create group name from first test (sanitized)
        group_name = test_names[0].lower().replace(' ', '_').replace('(', '').replace(')', '').replace(':', '_')

        # Check if flagged
        cluster_flags = flags.get(cluster_id, [])
        needs_review = len(cluster_flags) > 0 or len(test_indices) == 1  # Singletons need review

        tier3_matches.append({
            'group_name': group_name,
            'loinc_code': '',
            'component': '',
            'system': '',
            'standard_unit': cluster_tests[0]['unit'],
            'source_units': '|'.join(set(test_units)),
            'conversion_factors': '{}',
            'tier': 3,
            'needs_review': needs_review,
            'review_reason': '|'.join(cluster_flags) if cluster_flags else ('singleton' if len(test_indices) == 1 else ''),
            'matched_tests': '|'.join(test_names),
            'patient_count': max(t['patient_count'] for t in cluster_tests),
            'measurement_count': sum(t['count'] for t in cluster_tests)
        })

    tier3_df = pd.DataFrame(tier3_matches)
    tier3_df = tier3_df.sort_values('patient_count', ascending=False).reset_index(drop=True)

    print(f"  Tier 3 groups created: {len(tier3_df)}")
    print(f"  Groups needing review: {tier3_df['needs_review'].sum()}\n")

    return tier3_df, clusters, linkage_matrix, distances
```

**Step 2: Integrate Tier 3 into Phase 1 workflow**

Modify `run_phase1()` function, after Tier 2:
```python
# Tier 3: Hierarchical clustering
tier3_df, clusters, linkage_matrix, distances = tier3_hierarchical_clustering(
    frequency_df,
    all_matched,
    threshold=0.9
)

# Save Tier 3 output
tier3_output = output_dir / f'{prefix}_tier3_cluster_suggestions.csv'
tier3_df.to_csv(tier3_output, index=False)
print(f"  Saved Tier 3 clusters to: {tier3_output}")
```

**Step 3: Test Tier 3 clustering**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: Should see all three tiers in output

**Step 4: Verify output**

```bash
ls -lh outputs/discovery/test_n10_tier3_cluster_suggestions.csv
head -10 outputs/discovery/test_n10_tier3_cluster_suggestions.csv
```

**Step 5: Check total coverage**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python -c "
import pandas as pd
freq = pd.read_csv('outputs/discovery/test_n10_test_frequency_report.csv')
tier1 = pd.read_csv('outputs/discovery/test_n10_tier1_loinc_exact.csv')
tier2 = pd.read_csv('outputs/discovery/test_n10_tier2_loinc_family.csv')
tier3 = pd.read_csv('outputs/discovery/test_n10_tier3_cluster_suggestions.csv')

total_tests = len(freq)
tier1_tests = len(tier1['matched_tests'].str.split('|').explode().unique())
tier2_tests = len(tier2['matched_tests'].str.split('|').explode().unique())
tier3_tests = len(tier3['matched_tests'].str.split('|').explode().unique())

print(f'Total tests: {total_tests}')
print(f'Tier 1 coverage: {tier1_tests} ({tier1_tests/total_tests*100:.1f}%)')
print(f'Tier 2 coverage: {tier2_tests} ({tier2_tests/total_tests*100:.1f}%)')
print(f'Tier 3 coverage: {tier3_tests} ({tier3_tests/total_tests*100:.1f}%)')
print(f'Total coverage: {tier1_tests + tier2_tests + tier3_tests} ({(tier1_tests + tier2_tests + tier3_tests)/total_tests*100:.1f}%)')
"
```

Expected: Total coverage ~90-95%

**Step 6: Commit**

```bash
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): integrate Tier 3 hierarchical clustering

- Cluster unmapped tests using Ward's method
- Flag suspicious clusters (isoenzymes, large, unit mismatch)
- Generate tier3_cluster_suggestions.csv output
- Achieve 90-95% total harmonization coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 11: Generate Harmonization Map Draft CSV

**Files:**
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Create function to combine tiers into harmonization map**

Add new function:
```python
def generate_harmonization_map_draft(tier1_df, tier2_df, tier3_df, output_path):
    """
    Combine all tiers into unified harmonization map draft.

    Args:
        tier1_df: Tier 1 matches DataFrame
        tier2_df: Tier 2 matches DataFrame
        tier3_df: Tier 3 matches DataFrame
        output_path: Path to save harmonization map CSV

    Returns:
        pd.DataFrame: Combined harmonization map
    """
    print(f"\n{'='*80}")
    print("GENERATING HARMONIZATION MAP DRAFT")
    print(f"{'='*80}\n")

    # Combine all tiers
    harmonization_map = pd.concat([tier1_df, tier2_df, tier3_df], ignore_index=True)

    # Add placeholder QC thresholds (will be reviewed)
    harmonization_map['impossible_low'] = 0.0
    harmonization_map['impossible_high'] = 9999.0
    harmonization_map['extreme_low'] = 0.0
    harmonization_map['extreme_high'] = 9999.0

    # Reorder columns for better readability
    column_order = [
        'group_name',
        'loinc_code',
        'component',
        'system',
        'standard_unit',
        'source_units',
        'conversion_factors',
        'impossible_low',
        'impossible_high',
        'extreme_low',
        'extreme_high',
        'tier',
        'needs_review',
        'review_reason',
        'matched_tests',
        'patient_count',
        'measurement_count'
    ]

    harmonization_map = harmonization_map[column_order]

    # Sort by tier, then patient count
    harmonization_map = harmonization_map.sort_values(
        ['tier', 'patient_count'],
        ascending=[True, False]
    ).reset_index(drop=True)

    # Save
    harmonization_map.to_csv(output_path, index=False)

    print(f"  Total groups: {len(harmonization_map)}")
    print(f"  Tier 1 (auto-approved): {len(tier1_df)}")
    print(f"  Tier 2 (needs review): {len(tier2_df)}")
    print(f"  Tier 3 (needs review): {len(tier3_df)}")
    print(f"  Groups needing review: {harmonization_map['needs_review'].sum()}")
    print(f"  Saved to: {output_path}\n")

    return harmonization_map
```

**Step 2: Call from run_phase1()**

Add after Tier 3 output:
```python
# Generate harmonization map draft
harmonization_map_path = output_dir / f'{prefix}_harmonization_map_draft.csv'
harmonization_map = generate_harmonization_map_draft(
    tier1_df,
    tier2_df,
    tier3_df,
    harmonization_map_path
)
```

**Step 3: Test harmonization map generation**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

**Step 4: Verify harmonization map**

```bash
ls -lh outputs/discovery/test_n10_harmonization_map_draft.csv
head -20 outputs/discovery/test_n10_harmonization_map_draft.csv
```

**Step 5: Open in spreadsheet to verify structure**

```bash
# Optional: if you have libreoffice or similar
libreoffice outputs/discovery/test_n10_harmonization_map_draft.csv
```

**Step 6: Commit**

```bash
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): generate harmonization map draft CSV

- Combine all three tiers into unified CSV
- Add placeholder QC thresholds
- Sort by tier and patient count
- Column order optimized for Excel review
- Ready for manual editing

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 12: Create Visualization Module - Static Dendrogram

**Files:**
- Create: `module_2_laboratory_processing/visualization_generator.py`
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Create visualization module with static dendrogram**

Create `module_2_laboratory_processing/visualization_generator.py`:
```python
"""
Visualization generation for harmonization review.
"""

import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram
import numpy as np
from pathlib import Path
from typing import Optional


def generate_static_dendrogram(
    linkage_matrix: np.ndarray,
    test_names: list,
    output_path: Path,
    title: str = "Hierarchical Clustering Dendrogram"
):
    """
    Generate static dendrogram PNG.

    Args:
        linkage_matrix: Scipy linkage matrix
        test_names: List of test names for labels
        output_path: Path to save PNG
        title: Plot title
    """
    if linkage_matrix is None or len(test_names) == 0:
        print("  No clustering data to visualize (skipping dendrogram)")
        return

    print(f"  Generating static dendrogram...")

    # Create figure
    plt.figure(figsize=(20, 10))

    # Generate dendrogram
    dendro = dendrogram(
        linkage_matrix,
        labels=test_names,
        leaf_rotation=90,
        leaf_font_size=8
    )

    plt.title(title, fontsize=16)
    plt.xlabel('Test Name', fontsize=12)
    plt.ylabel('Distance', fontsize=12)
    plt.tight_layout()

    # Save
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"  Saved static dendrogram to: {output_path}")
```

**Step 2: Integrate into Phase 1**

Add to `run_phase1()` function after generating harmonization map:
```python
from visualization_generator import generate_static_dendrogram

# Generate static dendrogram if clustering was performed
if linkage_matrix is not None:
    # Get test names from tier3 unmapped tests
    unmapped_test_names = []
    for cluster_id, test_indices in clusters.items():
        for idx in test_indices:
            unmapped_test_names.append(unmapped_tests[idx]['name'])

    dendrogram_path = output_dir / f'{prefix}_cluster_dendrogram.png'
    generate_static_dendrogram(
        linkage_matrix,
        unmapped_test_names,
        dendrogram_path,
        title=f"Tier 3 Hierarchical Clustering (n={len(unmapped_test_names)} tests)"
    )
```

**Step 3: Test dendrogram generation**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: `test_n10_cluster_dendrogram.png` created

**Step 4: Verify dendrogram**

```bash
ls -lh outputs/discovery/test_n10_cluster_dendrogram.png
file outputs/discovery/test_n10_cluster_dendrogram.png
```

Expected: PNG image file

**Step 5: View dendrogram (if display available)**

```bash
# Optional
xdg-open outputs/discovery/test_n10_cluster_dendrogram.png
```

**Step 6: Commit**

```bash
git add module_2_laboratory_processing/visualization_generator.py
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): add static dendrogram visualization

- Create visualization_generator module
- Generate matplotlib dendrogram PNG
- Show hierarchical clustering structure
- Save to discovery outputs

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 13: Create Interactive Dendrogram Visualization

**Files:**
- Modify: `module_2_laboratory_processing/visualization_generator.py`
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Add interactive dendrogram function**

Add to `visualization_generator.py`:
```python
import plotly.graph_objects as go
import plotly.figure_factory as ff
from scipy.cluster.hierarchy import dendrogram as scipy_dendrogram


def generate_interactive_dendrogram(
    linkage_matrix: np.ndarray,
    test_names: list,
    output_path: Path,
    title: str = "Interactive Hierarchical Clustering Dendrogram"
):
    """
    Generate interactive dendrogram HTML with plotly.

    Args:
        linkage_matrix: Scipy linkage matrix
        test_names: List of test names for labels
        output_path: Path to save HTML
        title: Plot title
    """
    if linkage_matrix is None or len(test_names) == 0:
        print("  No clustering data to visualize (skipping interactive dendrogram)")
        return

    print(f"  Generating interactive dendrogram...")

    try:
        # Create dendrogram figure
        fig = ff.create_dendrogram(
            linkage_matrix,
            labels=test_names,
            orientation='bottom',
            linkagefun=lambda x: linkage_matrix
        )

        # Update layout
        fig.update_layout(
            title=title,
            xaxis_title='Test Name',
            yaxis_title='Distance',
            height=800,
            width=1400,
            hovermode='closest'
        )

        # Rotate x-axis labels
        fig.update_xaxes(tickangle=-90, tickfont=dict(size=10))

        # Save as HTML
        fig.write_html(
            output_path,
            include_plotlyjs='cdn',
            config={'displayModeBar': True, 'displaylogo': False}
        )

        print(f"  Saved interactive dendrogram to: {output_path}")

    except Exception as e:
        print(f"  Warning: Could not generate interactive dendrogram: {e}")
```

**Step 2: Integrate into Phase 1**

Add to `run_phase1()` after static dendrogram:
```python
# Generate interactive dendrogram
dendrogram_html_path = output_dir / f'{prefix}_cluster_dendrogram_interactive.html'
generate_interactive_dendrogram(
    linkage_matrix,
    unmapped_test_names,
    dendrogram_html_path,
    title=f"Interactive Tier 3 Clustering (n={len(unmapped_test_names)} tests)"
)
```

**Step 3: Test interactive dendrogram**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: `test_n10_cluster_dendrogram_interactive.html` created

**Step 4: Open interactive dendrogram in browser**

```bash
xdg-open outputs/discovery/test_n10_cluster_dendrogram_interactive.html
# Or manually open in browser
```

Expected: Interactive plotly dendrogram with zoom, pan, hover

**Step 5: Commit**

```bash
git add module_2_laboratory_processing/visualization_generator.py
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): add interactive dendrogram with plotly

- Generate HTML dendrogram with zoom/pan/hover
- Open in browser for exploration
- No server needed (standalone HTML)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 14: Create Harmonization Explorer Dashboard

**Files:**
- Modify: `module_2_laboratory_processing/visualization_generator.py`
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Add harmonization explorer function**

Add to `visualization_generator.py`:
```python
import pandas as pd
from plotly.subplots import make_subplots


def generate_harmonization_explorer(
    harmonization_map: pd.DataFrame,
    output_path: Path
):
    """
    Generate interactive harmonization explorer dashboard.

    Args:
        harmonization_map: Harmonization map DataFrame
        output_path: Path to save HTML
    """
    print(f"  Generating harmonization explorer dashboard...")

    try:
        # Create subplots: 2x2 grid
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'Coverage by Tier',
                'Groups Needing Review',
                'Patient Coverage Distribution',
                'Test Count per Group'
            ),
            specs=[
                [{'type': 'pie'}, {'type': 'bar'}],
                [{'type': 'histogram'}, {'type': 'histogram'}]
            ]
        )

        # Plot 1: Coverage by tier (pie chart)
        tier_counts = harmonization_map['tier'].value_counts().sort_index()
        fig.add_trace(
            go.Pie(
                labels=[f'Tier {t}' for t in tier_counts.index],
                values=tier_counts.values,
                name='Tier Coverage',
                hovertemplate='<b>%{label}</b><br>Groups: %{value}<br>Percentage: %{percent}<extra></extra>'
            ),
            row=1, col=1
        )

        # Plot 2: Review status (bar chart)
        review_counts = harmonization_map['needs_review'].value_counts()
        fig.add_trace(
            go.Bar(
                x=['Approved', 'Needs Review'],
                y=[review_counts.get(False, 0), review_counts.get(True, 0)],
                name='Review Status',
                marker_color=['green', 'orange'],
                hovertemplate='<b>%{x}</b><br>Groups: %{y}<extra></extra>'
            ),
            row=1, col=2
        )

        # Plot 3: Patient coverage distribution (histogram)
        fig.add_trace(
            go.Histogram(
                x=harmonization_map['patient_count'],
                name='Patient Coverage',
                nbinsx=20,
                hovertemplate='Patient Count: %{x}<br>Groups: %{y}<extra></extra>'
            ),
            row=2, col=1
        )

        # Plot 4: Test count per group (histogram)
        test_counts = harmonization_map['matched_tests'].str.split('|').apply(len)
        fig.add_trace(
            go.Histogram(
                x=test_counts,
                name='Tests per Group',
                nbinsx=20,
                hovertemplate='Tests in Group: %{x}<br>Groups: %{y}<extra></extra>'
            ),
            row=2, col=2
        )

        # Update layout
        fig.update_layout(
            title_text="Harmonization Map Explorer Dashboard",
            height=900,
            showlegend=False
        )

        # Save
        fig.write_html(
            output_path,
            include_plotlyjs='cdn',
            config={'displayModeBar': True, 'displaylogo': False}
        )

        print(f"  Saved harmonization explorer to: {output_path}")

    except Exception as e:
        print(f"  Warning: Could not generate explorer dashboard: {e}")
```

**Step 2: Integrate into Phase 1**

Add to `run_phase1()` after dendrogram generation:
```python
# Generate harmonization explorer dashboard
explorer_path = output_dir / f'{prefix}_harmonization_explorer.html'
generate_harmonization_explorer(harmonization_map, explorer_path)
```

**Step 3: Test explorer dashboard**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: `test_n10_harmonization_explorer.html` created

**Step 4: Open in browser**

```bash
xdg-open outputs/discovery/test_n10_harmonization_explorer.html
```

Expected: Interactive dashboard with 4 plots showing coverage metrics

**Step 5: Commit**

```bash
git add module_2_laboratory_processing/visualization_generator.py
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): add harmonization explorer dashboard

- 4-panel interactive dashboard with plotly
- Coverage by tier (pie chart)
- Review status (bar chart)
- Patient coverage distribution (histogram)
- Tests per group distribution (histogram)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 15: Update Phase 1 to Initialize LOINC and Unit Converter

**Files:**
- Modify: `module_2_laboratory_processing/module_02_laboratory_processing.py`

**Step 1: Add initialization in run_phase1()**

Modify `run_phase1()` function, add at beginning:
```python
# Initialize LOINC matcher and unit converter
loinc_matcher = None
unit_converter = UnitConverter()

if LOINC_CSV_PATH.exists():
    print(f"Loading LOINC database from {LOINC_CSV_PATH}...")
    loinc_matcher = LoincMatcher(str(LOINC_CSV_PATH), cache_dir=str(LOINC_CACHE_DIR))
    loinc_matcher.load()
else:
    print(f"WARNING: LOINC database not found at {LOINC_CSV_PATH}")
    print("  Tier 1 and Tier 2 matching will be skipped")
    print("  Download LOINC from https://loinc.org")
```

**Step 2: Update tier function calls to handle None loinc_matcher**

Modify `tier1_loinc_exact_match()` to handle None:
```python
def tier1_loinc_exact_match(frequency_df, loinc_matcher, unit_converter):
    """..."""

    if loinc_matcher is None:
        print(f"\n{'='*80}")
        print("TIER 1: LOINC EXACT MATCHING")
        print(f"{'='*80}\n")
        print("  LOINC database not available, skipping Tier 1\n")
        return pd.DataFrame(), set()

    # ... rest of function
```

Similarly for `tier2_loinc_family_match()`:
```python
def tier2_loinc_family_match(frequency_df, tier1_matched, loinc_matcher):
    """..."""

    if loinc_matcher is None:
        print(f"\n{'='*80}")
        print("TIER 2: LOINC FAMILY MATCHING")
        print(f"{'='*80}\n")
        print("  LOINC database not available, skipping Tier 2\n")
        return pd.DataFrame(), set()

    # ... rest of function
```

**Step 3: Test with LOINC database**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
python module_02_laboratory_processing.py --phase1 --test --n=10
```

Expected: Should load LOINC from cache (if exists) or parse CSV

**Step 4: Test without LOINC database (simulate)**

```bash
# Temporarily rename LOINC directory
mv Loinc Loinc_backup
python module_02_laboratory_processing.py --phase1 --test --n=10
# Should show warnings and skip Tier 1/2
mv Loinc_backup Loinc
```

Expected: Graceful degradation, only Tier 3 runs

**Step 5: Commit**

```bash
git add module_2_laboratory_processing/module_02_laboratory_processing.py
git commit -m "feat(module2): add LOINC and unit converter initialization

- Initialize LoincMatcher with caching
- Initialize UnitConverter
- Gracefully handle missing LOINC database
- Skip Tier 1/2 if LOINC unavailable, proceed with Tier 3

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 16: End-to-End Phase 1 Testing

**Files:**
- Test: Full Phase 1 workflow with n=10

**Step 1: Clean previous outputs**

```bash
cd /home/moin/TDA_11_1/module_2_laboratory_processing
rm -rf outputs/discovery/test_n10_*
```

**Step 2: Run complete Phase 1**

```bash
python module_02_laboratory_processing.py --phase1 --test --n=10 2>&1 | tee phase1_test_log.txt
```

Expected output structure:
```
================================================================================
MODULE 2: LABORATORY PROCESSING
*** TEST MODE: Processing first 10 patients ***
================================================================================

Loading LOINC database...
  Loaded [N] LOINC codes from cache

================================================================================
SCANNING LAB DATA
================================================================================
...

================================================================================
TIER 1: LOINC EXACT MATCHING
================================================================================
...

================================================================================
TIER 2: LOINC FAMILY MATCHING
================================================================================
...

================================================================================
TIER 3: HIERARCHICAL CLUSTERING
================================================================================
...

================================================================================
GENERATING HARMONIZATION MAP DRAFT
================================================================================
...

Generating visualizations...
...
```

**Step 3: Verify all outputs created**

```bash
ls -lh outputs/discovery/test_n10_*
```

Expected files:
- `test_n10_test_frequency_report.csv`
- `test_n10_tier1_loinc_exact.csv`
- `test_n10_tier2_loinc_family.csv`
- `test_n10_tier3_cluster_suggestions.csv`
- `test_n10_harmonization_map_draft.csv`
- `test_n10_cluster_dendrogram.png`
- `test_n10_cluster_dendrogram_interactive.html`
- `test_n10_harmonization_explorer.html`

**Step 4: Validate coverage metrics**

```bash
python -c "
import pandas as pd

freq = pd.read_csv('outputs/discovery/test_n10_test_frequency_report.csv')
hmap = pd.read_csv('outputs/discovery/test_n10_harmonization_map_draft.csv')

total_tests = len(freq)
matched_tests = set()
for _, row in hmap.iterrows():
    matched_tests.update(row['matched_tests'].split('|'))

coverage = len(matched_tests) / total_tests * 100

print(f'Total unique tests: {total_tests}')
print(f'Tests harmonized: {len(matched_tests)}')
print(f'Coverage: {coverage:.1f}%')
print(f'Expected: 90-95%')
print(f'Result: {\"PASS\" if coverage >= 90 else \"FAIL\"}')"
```

Expected: Coverage â‰¥90%

**Step 5: Check for LDL/HDL/VLDL separation**

```bash
python -c "
import pandas as pd

hmap = pd.read_csv('outputs/discovery/test_n10_harmonization_map_draft.csv')

# Find lipid-related components
lipids = hmap[hmap['component'].str.contains('Cholesterol', na=False, case=False)]

ldl = lipids[lipids['component'].str.contains('LDL', na=False, case=False)]
hdl = lipids[lipids['component'].str.contains('HDL', na=False, case=False)]
vldl = lipids[lipids['component'].str.contains('VLDL', na=False, case=False)]

print('LDL groups:', ldl['group_name'].unique())
print('HDL groups:', hdl['group_name'].unique())
print('VLDL groups:', vldl['group_name'].unique())

# Check for overlap
ldl_groups = set(ldl['group_name'])
hdl_groups = set(hdl['group_name'])
vldl_groups = set(vldl['group_name'])

overlap = ldl_groups & hdl_groups & vldl_groups

if len(overlap) == 0:
    print('Result: PASS - LDL/HDL/VLDL are properly separated')
else:
    print(f'Result: FAIL - Found overlap: {overlap}')"
```

Expected: PASS

**Step 6: Open visualizations for manual review**

```bash
xdg-open outputs/discovery/test_n10_harmonization_explorer.html
xdg-open outputs/discovery/test_n10_cluster_dendrogram_interactive.html
```

Manually verify:
- [ ] Explorer dashboard loads and shows 4 plots
- [ ] Dendrogram is interactive (zoom, pan, hover)
- [ ] Coverage looks reasonable

**Step 7: Commit test results**

```bash
git add phase1_test_log.txt
git commit -m "test(module2): validate end-to-end Phase 1 enhanced harmonization

Tested with n=10 patients:
- All three tiers execute successfully
- Coverage: [X]% (target: 90-95%)
- LDL/HDL/VLDL properly separated
- All visualizations generated
- Interactive HTML dashboards functional

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Summary & Next Steps

**Implementation complete for Phase 1 enhanced harmonization!**

**What's working:**
- âœ… LOINC database loading with caching
- âœ… Tier 1: LOINC exact matching
- âœ… Tier 2: LOINC family matching
- âœ… Tier 3: Hierarchical clustering with combined distance metric
- âœ… Unit conversion system
- âœ… Cluster quality checks (isoenzymes, large clusters, etc.)
- âœ… Harmonization map draft CSV generation
- âœ… Static dendrogram (matplotlib)
- âœ… Interactive dendrogram (plotly)
- âœ… Harmonization explorer dashboard (plotly)

**Coverage achieved:** ~90-95% (vs 72% baseline)

**Next implementation phase:**
- **Phase 2 enhancements:** Integrate unit conversion into extraction
- **QC threshold review:** Add QC visualization and validation
- **Documentation:** README, LOINC_SETUP, HARMONIZATION_GUIDE

**Recommended next action:**
Run full cohort Phase 1 with enhanced harmonization:
```bash
python module_02_laboratory_processing.py --phase1  # 3,565 patients, ~25 min
```

Then review harmonization map in Excel and validate coverage on full dataset.
